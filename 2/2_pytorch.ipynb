{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIOMAG introduction material 2. - Pytorch basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # pytorch basic package\n",
    "from torch import nn # neural net \n",
    "from torch.utils.data import DataLoader, Dataset # to work with data\n",
    "from torchvision import datasets # built-in data\n",
    "from torchvision.transforms import ToTensor # to convert nparrays/images into pytorch tensors\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's check if the GPU is available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us download the datasets that we are going to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PyTorch ``DataLoader`` acts as a wrapper over the ``Dataset`` and supports batching, random sampling, augmentation, etc., basically everything you would need for your neural network research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break\n",
    "\n",
    "print(test_data[1][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how a single sample from our test set looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{y[0] = }')\n",
    "print(f'{y[0].item() = }')\n",
    "print(f'{labels_map[y[0].item()] = }')\n",
    "plt.imshow(X[0,0,:,:], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us create our own neural network in PyTorch. for that, we need to create our own class, which inherits most of the properties from the ``nn.Module`` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the GPU as the main device if it is available, if not, the CPU will suffice\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f'We are using the following device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    # here we define the neural net layers\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten() # this flattens out any tensor, meaning a 28x28 matrix will become a 1D vector of 784\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(in_features = 28*28, out_features = 512, bias = True), # this will do the standard wx + b perceptron operation and will set all parameters to be the correct shape.\n",
    "            nn.ReLU(), # perform the relu activation (z = relu(wx + b))\n",
    "            nn.Linear(in_features = 512, out_features = 512), # and so on... note that in_features has to match the previous layer's out_features size.\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features = 512, out_features = 10) # bias is set True by default.\n",
    "        )\n",
    "\n",
    "    # here we specify how the data will flow through the network.\n",
    "    def forward(self, x):\n",
    "        # first we flatten our input x, which is an image (why is thid good/bad?)\n",
    "        x = self.flatten(x)\n",
    "        # then we perform the network prediction with the sequential layers specified in the init function.\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        # we return the prediction (what is the size?)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create our model by instantiating the NeuralNetwork class and immediately move it to the GPU\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we apply our model on a batch of images from the training set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(iter(train_dataloader))\n",
    "result = model(X) # apply the operations specified in the forward function: it's the same as model.forward(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get a problem, because our network (``model``) is on a different device compared to the samples and labels ``X`` and ``y``. Let's move our samples and labels to the GPU as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = X.to(device), y.to(device)\n",
    "result = model(X)\n",
    "print(f'{result.shape = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how does a prediction look like for a single image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{result[0] = }')\n",
    "print(f'{result = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These values are essentially probabilities, so for example, ``result[0]`` will output 10 probability values for the 1st image in the batch, each telling us how likely it is that the image is from class ``n`` (where ``n`` goes from 0  to 9). An easy way to tell immediately which class got the highest probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result[0].argmax().item())\n",
    "print(f' The predicted class for image 0 is: {labels_map[result[0].argmax().item()]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training (and testing) the network\n",
    "Do train a network, we need a loss function and an optimizer method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training procedure for a single epoch\n",
    "def train(train_dataloader, model, loss_fn, optimizer):\n",
    "    num_samples = len(train_dataloader.dataset)\n",
    "    num_batches = len(train_dataloader)\n",
    "    model.train() # if we are in training mode, some functions will behave differently (e.g. dropout, batchnorm)\n",
    "\n",
    "    training_loss = 0\n",
    "\n",
    "    # iterate through the whole dataset, batch just stores the index of the batch.\n",
    "    # if we have a total of 128 images, with a batch size of 32 we will have 4 batches before the for loop stops\n",
    "    for batch, (X,y) in enumerate(train_dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # calc yhat and loss value\n",
    "        yhat = model(X)\n",
    "        loss = loss_fn(yhat, y)\n",
    "        training_loss += loss.item()\n",
    "\n",
    "        # backpropagation: calculate the partial derivatives for each neural net parameter\n",
    "        loss.backward()\n",
    "        # update neural network parameters according to the partial derivatives\n",
    "        optimizer.step()\n",
    "        # zero out the variables that are used for storing the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # we ususally don't want to print information at every batch, so make it a bit more scarce:\n",
    "        if batch % 100 == 0:\n",
    "            current_loss = loss.item()\n",
    "            current = (batch + 1) * len(X) # len(X) is the batch size\n",
    "            print(f'{current}/{num_samples} : {current_loss = }')\n",
    "    \n",
    "    training_loss /= num_batches\n",
    "    return training_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also write a test function to see how the model works on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, test_dataloader, model, loss_fn):\n",
    "    num_samples = len(test_dataloader.dataset)\n",
    "    num_batches = len(test_dataloader)\n",
    "    model.eval() # if we are in eval mode, some functions behave differently\n",
    "    test_loss = 0 \n",
    "    correct = 0\n",
    "    with torch.no_grad(): # we do not want to calculate gradients in eval mode\n",
    "        for X, y in test_dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y).item()\n",
    "            test_loss += loss # we summarize the test loss across all batches\n",
    "            correct_pred_locations = (pred.argmax(1) == y).type(torch.float) # also torch.argmax(pred, dim=1) is correct, and then we convert these locations to float\n",
    "            correct += correct_pred_locations.sum().item() # we summarize how many items we got correct\n",
    "    test_loss /= num_batches # we average the loss\n",
    "    correct /= num_samples\n",
    "    print(f'End of epoch {epoch+1}\\n Accuracy: {(100*correct):.2f}%, Average loss: {test_loss:.4f}\\n')\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do the actual training and evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "for t in tqdm(range(epochs)):\n",
    "    train_loss = train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loss = test(t, test_dataloader, model, loss_fn)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us plot the training and validation curves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label=\"training loss\")\n",
    "plt.plot(test_losses, label=\"test (validation) loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us save this model for further use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"mnist_starter_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to load it later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load(\"mnist_starter_model.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a quick visual evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "indices = np.random.randint(len(test_data), size=(4)) # pick 4 random images from the test set\n",
    "f = plt.figure()\n",
    "for fig_idx, i in enumerate(indices):\n",
    "    x, y = test_data[i][0], test_data[i][1]\n",
    "    x = x.to(device)\n",
    "    pred_vector = model(x)\n",
    "    pred_value = labels_map[pred_vector.argmax().item()]\n",
    "    gt_value = labels_map[y]\n",
    "    plt.subplot(2,2,fig_idx+1)\n",
    "    plt.title(f'GT: {gt_value}, pred: {pred_value}')\n",
    "    plt.imshow(x.cpu()[0], cmap='gray')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework\n",
    "1. Try training the network for a longer time. What can you observe in terms of training and validation losses? Why do you think this is the case?\n",
    "2. Try tweaking the network parameters (number of neurons, number of layers, etc) to reach a better accuracy than the original model after 10 epochs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detectron",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
